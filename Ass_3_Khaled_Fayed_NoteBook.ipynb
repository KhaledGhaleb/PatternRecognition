{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass_3_Khaled_Fayed_NoteBook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmgFOSL3pgrm+D2NsheqkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaledGhaleb/PatternRecognition/blob/main/Ass_3_Khaled_Fayed_NoteBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc5MTwGHgjPG"
      },
      "source": [
        "# **Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72jZl8q6mZii"
      },
      "source": [
        "Get Corel Reduced Data from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYd4LO3kmop9",
        "outputId": "90b3b736-cc97-4ba9-da86-242f862e1f29"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "_URL = 'https://raw.githubusercontent.com/KhaledGhaleb/CorelDataSet/main/CORELREduced.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('COREL-REduced.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'COREL-REduced')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/KhaledGhaleb/CorelDataSet/main/CORELREduced.zip\n",
            "2220032/2215519 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35A6vQtMmCMc"
      },
      "source": [
        "# 1- Pretrained VGG16 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV2ZX4rnygil"
      },
      "source": [
        "Prepare Train and Validation data:\n",
        "\n",
        "set ratio of 0.5 validation = 20 * 0.5 = 10 validation\n",
        "\n",
        "Set Image size to 224*224 * 3 to support VGG16\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uBTtluWyfnR",
        "outputId": "16726c82-3645-4322-83ad-c6ae1375e644"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (224, 224)\n",
        "COLOR_MODE = 'rgb' #\"grayscale\"  #  \"rgb\"\n",
        "Color_Num = 3\n",
        "Valid_ratio = 0.5\n",
        "\n",
        "train_dataset_VGG16 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = \"training\",\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )\n",
        "Validation_dataset_VGG16 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = 'validation',\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for training.\n",
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPBRTUcHxICN"
      },
      "source": [
        "Normalize data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMtJYBq7xICS"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#reshape_layer = tf.keras.layers.Reshape(3, 4)\n",
        "train_normalized_ds_VGG16 = train_dataset_VGG16.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_normalized_ds_VGG16 = Validation_dataset_VGG16.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlzDwLbxICT"
      },
      "source": [
        "Convert normalized dataset to images and labels in nparray "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVtFPXKVxICU"
      },
      "source": [
        "train_images_VGG16,train_labels_VGG16  = list(train_normalized_ds_VGG16.as_numpy_iterator())[0]\n",
        "validate_images_VGG16,validate_labels_VGG16  = list(validation_normalized_ds_VGG16.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO-vn6sSkZK4"
      },
      "source": [
        "#tf.keras.layers.experimental.preprocessing.Resizing( 293, 293, interpolation=\"bilinear\", crop_to_aspect_ratio=False)\n",
        "#train_images_GoogleNet = train_images.resize(293,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySzffFj9gn51"
      },
      "source": [
        "Generate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVogAKMFgW-z",
        "outputId": "21a1cdf2-dc31-4788-a3a0-df295c78dd6a"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "model_VGG16 = VGG16(weights='imagenet') #,include_top=False\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PYj3qPb4Pix"
      },
      "source": [
        "Create Feautre Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OLTMmLGXwXy"
      },
      "source": [
        "TrainFeaturesArr_VGG16 = model_VGG16.predict(train_images_VGG16)\n",
        "ValidateFeaturesArr_VGG16 = model_VGG16.predict(validate_images_VGG16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sqrdGwc4kM"
      },
      "source": [
        "Add KNN layer with K = 3 and P = 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tWTwBCUbIcy"
      },
      "source": [
        "from sklearn import neighbors\n",
        "knn_VGG16 = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
        "knn_VGG16.fit(TrainFeaturesArr_VGG16, train_labels_VGG16)\n",
        "y_pred_VGG16 = knn_VGG16.predict(ValidateFeaturesArr_VGG16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWfIZXZfbqzt",
        "outputId": "6b7f490f-66e7-4e67-8ec8-e00e9435b1fc"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print('VGG16 Accuracy score: ',accuracy_score(y_pred_VGG16, validate_labels_VGG16))\n",
        "print('VGG16 F1 Score: ',f1_score(y_pred_VGG16, validate_labels_VGG16, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16 Accuracy score:  0.825\n",
            "VGG16 F1 Score:  0.845711500974659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AltmnzSVgrkn"
      },
      "source": [
        "# 2- Pretrained GoogleNet (InceptionV3)\n",
        "using InceptionV3 pretrained with imagenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IuFTA7uoGGL"
      },
      "source": [
        "Prepare Train and Validation data:\n",
        "\n",
        "set ratio of 0.5 validation = 20 * 0.5 = 10 validation\n",
        "\n",
        "Set Image size to 299*299 * 3 to support InceptionV3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghk0Gq9EoGGc",
        "outputId": "f2da88ee-cbdf-482f-e470-0f8492d9cc63"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (299, 299)\n",
        "COLOR_MODE = 'rgb' #\"grayscale\"  #  \"rgb\"\n",
        "Color_Num = 3\n",
        "Valid_ratio = 0.5\n",
        "\n",
        "train_dataset_InceptionV3 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = \"training\",\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )\n",
        "Validation_dataset_InceptionV3 = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = 'validation',\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for training.\n",
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Do_6zmqoGGe"
      },
      "source": [
        "Normalize data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ7ZaeKQoGGe"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#reshape_layer = tf.keras.layers.Reshape(3, 4)\n",
        "train_normalized_ds_InceptionV3 = train_dataset_InceptionV3.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_normalized_ds_InceptionV3 = Validation_dataset_InceptionV3.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n5xUaFNoGGj"
      },
      "source": [
        "Convert normalized dataset to images and labels in nparray "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6OhI00soGGk"
      },
      "source": [
        "train_images_InceptionV3,train_labels_InceptionV3  = list(train_normalized_ds_InceptionV3.as_numpy_iterator())[0]\n",
        "validate_images_InceptionV3,validate_labels_InceptionV3  = list(validation_normalized_ds_InceptionV3.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H0QgkG1oGGl"
      },
      "source": [
        "Generate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAMXfrOmoGGl",
        "outputId": "40719cb5-e38f-4ceb-ba6e-b667616cf350"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3  import InceptionV3\n",
        "model_InceptionV3 = InceptionV3(weights='imagenet') #,include_top=False\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9JzCSdKoGGm"
      },
      "source": [
        "Create Feautre Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-khQKd4oGGm"
      },
      "source": [
        "TrainFeaturesArr_InceptionV3 = model_InceptionV3.predict(train_images_InceptionV3)\n",
        "ValidateFeaturesArr_InceptionV3 = model_InceptionV3.predict(validate_images_InceptionV3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWi58C9voGGn"
      },
      "source": [
        "Add KNN layer with K = 3 and P = 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct_Zb68foGGo"
      },
      "source": [
        "from sklearn import neighbors\n",
        "knn_InceptionV3 = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
        "knn_InceptionV3.fit(TrainFeaturesArr_InceptionV3, train_labels_InceptionV3)\n",
        "y_pred_InceptionV3 = knn_InceptionV3.predict(ValidateFeaturesArr_InceptionV3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySsmlG32oGGo",
        "outputId": "dfe3d6ac-90b6-4680-b84f-9d269bbfc431"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print('InceptionV3 Accuracy score: ',accuracy_score(y_pred_InceptionV3, validate_labels_InceptionV3))\n",
        "print('InceptionV3 F1 Score: ',f1_score(y_pred_InceptionV3, validate_labels_InceptionV3, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InceptionV3 Accuracy score:  0.975\n",
            "InceptionV3 F1 Score:  0.9891304347826086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRNGOc9fsXK"
      },
      "source": [
        "# 3- Use VGG16 in a transfer learning mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvGZublzp8EY"
      },
      "source": [
        "Create VGG16 Model and Freeze this layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lTqwCbPfr-6",
        "outputId": "fb77d361-533c-4e64-c85d-e5b4109d209d"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "model_VGG16_Transfer =  VGG16(weights='imagenet',include_top=False)\n",
        "\n",
        "#freeze the transfer model.\n",
        "model_VGG16_Transfer.trainable = False\n",
        "\n",
        "model_VGG16_Transfer.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYzavDiqSvu"
      },
      "source": [
        "*Create* New Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7ypMG52w9l1",
        "outputId": "c796d3b7-6e58-46bc-8e98-7629a00f2328"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "Transfer_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=((240,240,3))),\n",
        "        model_VGG16_Transfer,\n",
        "        #VGG16(weights='imagenet',include_top=False),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        #layers.Dense(1024,activation='relu'),\n",
        "        #layers.Dense(512,activation='relu'),\n",
        "        layers.Dense(128,activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(4, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "Transfer_model.summary()\n",
        "for i, layer in enumerate(Transfer_model.layers):\n",
        "   print(i, layer.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "module_wrapper (ModuleWrappe (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 14,780,868\n",
            "Trainable params: 66,180\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "0 module_wrapper\n",
            "1 global_average_pooling2d\n",
            "2 dense\n",
            "3 dropout\n",
            "4 dense_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYNbQKA1wMoT",
        "outputId": "c0f9bdca-f716-43f7-e8bf-85065393cd44"
      },
      "source": [
        "Transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "Transfer_model.fit(x= train_images_VGG16, y= train_labels_VGG16,\n",
        "                epochs=25,\n",
        "                batch_size=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(validate_images_VGG16,validate_labels_VGG16))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.6558 - accuracy: 0.1271WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "4/4 [==============================] - 15s 314ms/step - loss: 1.6619 - accuracy: 0.1267 - val_loss: 1.5562 - val_accuracy: 0.2750\n",
            "Epoch 2/25\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 1.5102 - accuracy: 0.2933 - val_loss: 1.3750 - val_accuracy: 0.3500\n",
            "Epoch 3/25\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 1.3757 - accuracy: 0.4067 - val_loss: 1.1765 - val_accuracy: 0.5250\n",
            "Epoch 4/25\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 1.2933 - accuracy: 0.3933 - val_loss: 1.0506 - val_accuracy: 0.8500\n",
            "Epoch 5/25\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 1.1571 - accuracy: 0.4133 - val_loss: 0.9366 - val_accuracy: 0.9500\n",
            "Epoch 6/25\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 0.8622 - accuracy: 0.7467 - val_loss: 0.8688 - val_accuracy: 0.8750\n",
            "Epoch 7/25\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.8523 - accuracy: 0.8233 - val_loss: 0.8073 - val_accuracy: 0.8750\n",
            "Epoch 8/25\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.7731 - accuracy: 0.7833 - val_loss: 0.7338 - val_accuracy: 0.8750\n",
            "Epoch 9/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.7463 - accuracy: 0.8167 - val_loss: 0.6572 - val_accuracy: 0.9000\n",
            "Epoch 10/25\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.6435 - accuracy: 0.8767 - val_loss: 0.5966 - val_accuracy: 0.9250\n",
            "Epoch 11/25\n",
            "4/4 [==============================] - 0s 116ms/step - loss: 0.5759 - accuracy: 0.8900 - val_loss: 0.5446 - val_accuracy: 0.9250\n",
            "Epoch 12/25\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.5150 - accuracy: 0.8533 - val_loss: 0.4975 - val_accuracy: 0.9250\n",
            "Epoch 13/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.4415 - accuracy: 1.0000 - val_loss: 0.4563 - val_accuracy: 0.9250\n",
            "Epoch 14/25\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 0.4703 - accuracy: 0.9200 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
            "Epoch 15/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.4702 - accuracy: 0.9733 - val_loss: 0.4069 - val_accuracy: 0.9250\n",
            "Epoch 16/25\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 0.3512 - accuracy: 0.9367 - val_loss: 0.3784 - val_accuracy: 0.9250\n",
            "Epoch 17/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.3649 - accuracy: 0.9433 - val_loss: 0.3493 - val_accuracy: 0.9250\n",
            "Epoch 18/25\n",
            "4/4 [==============================] - 0s 120ms/step - loss: 0.4344 - accuracy: 0.8900 - val_loss: 0.3247 - val_accuracy: 0.9500\n",
            "Epoch 19/25\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 0.2855 - accuracy: 0.9833 - val_loss: 0.3052 - val_accuracy: 0.9250\n",
            "Epoch 20/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.3025 - accuracy: 0.9200 - val_loss: 0.2950 - val_accuracy: 0.9250\n",
            "Epoch 21/25\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 0.1935 - accuracy: 0.9900 - val_loss: 0.2871 - val_accuracy: 0.9250\n",
            "Epoch 22/25\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.2051 - accuracy: 0.9900 - val_loss: 0.2731 - val_accuracy: 0.9250\n",
            "Epoch 23/25\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.2199 - accuracy: 0.9833 - val_loss: 0.2576 - val_accuracy: 0.9250\n",
            "Epoch 24/25\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.1646 - accuracy: 0.9833 - val_loss: 0.2539 - val_accuracy: 0.9500\n",
            "Epoch 25/25\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 0.1831 - accuracy: 1.0000 - val_loss: 0.2480 - val_accuracy: 0.9500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2bc9f4c950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyqWRIat6x-P"
      },
      "source": [
        "Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6tYk2F86k2z",
        "outputId": "efb98c7e-9ee9-409f-d301-695a67081346"
      },
      "source": [
        "# Unfreeze the base model\n",
        "model_VGG16_Transfer.trainable = True\n",
        "Transfer_model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
        "                       loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "Transfer_model.fit(x= train_images_VGG16, y= train_labels_VGG16,\n",
        "                epochs=3,\n",
        "                batch_size=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(validate_images_VGG16,validate_labels_VGG16))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9729WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (10, 224, 224, 3).\n",
            "4/4 [==============================] - 5s 371ms/step - loss: 0.1767 - accuracy: 0.9733 - val_loss: 0.1634 - val_accuracy: 0.9250\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 1s 219ms/step - loss: 0.1496 - accuracy: 0.9533 - val_loss: 0.1969 - val_accuracy: 0.9000\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 1s 253ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2bc9f3f310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OSI3g_A7RNe"
      },
      "source": [
        "Scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIQq5lg67QXA",
        "outputId": "bba24cb0-453b-4f21-fc10-7b29c6cf0f9e"
      },
      "source": [
        "scores = Transfer_model.evaluate(validate_images_VGG16,validate_labels_VGG16, verbose=1) \n",
        "print(\"Transfer VGG16 Accuracy: \", scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 240, 240, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 224, 224, 3).\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.1571 - accuracy: 0.9000\n",
            "Transfer VGG16 Accuracy:  0.8999999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXnIowV719B"
      },
      "source": [
        "# 4- DCT2 traditional features and the KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkZsUgi28Qwi"
      },
      "source": [
        "Get DCT2 Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu-WbDJg79QH"
      },
      "source": [
        "from scipy.fftpack import  dct\n",
        "DCT2_TrainFeaturesArr = []\n",
        "for image in train_images_VGG16:\n",
        "  DCT_out = dct(image, 2)\n",
        "  DCT_out_1 = DCT_out[:10,:10] \n",
        "  DCT_out_2 = DCT_out_1.reshape((300))\n",
        "  DCT2_TrainFeaturesArr.append(DCT_out_2)\n",
        "\n",
        "DCT2_ValidateFeaturesArr = []\n",
        "for image in validate_images_VGG16:\n",
        "  DCT_out = dct(image, 2)\n",
        "  DCT_out_1 = DCT_out[:10,:10] \n",
        "  DCT_out_2 = DCT_out_1.reshape((300))\n",
        "  DCT2_ValidateFeaturesArr.append(DCT_out_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3i29yFE_pTm"
      },
      "source": [
        "Add KNN layer with K = 3 and P = 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr45kj2G_pTr"
      },
      "source": [
        "from sklearn import neighbors\n",
        "knn_DCT2 = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
        "knn_DCT2.fit(DCT2_TrainFeaturesArr, train_labels_VGG16)\n",
        "y_pred_DCT2 = knn_DCT2.predict(DCT2_ValidateFeaturesArr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZL5FDDf_pTu",
        "outputId": "1eeb564f-6eb8-4f2b-fbda-a1ba8153fe91"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "print('DCT2 KNN Accuracy score: ',accuracy_score(y_pred_DCT2, validate_labels_VGG16))\n",
        "print('DCT2 KNN F1 Score: ',f1_score(y_pred_DCT2, validate_labels_VGG16, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DCT2 KNN Accuracy score:  0.55\n",
            "DCT2 KNN F1 Score:  0.5553030303030303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jua23EYfyrb3"
      },
      "source": [
        "# 5- Train a CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3fYWfPvBmW6"
      },
      "source": [
        "Prepare Train and Validation data:\n",
        "\n",
        "set ratio of 0.5 validation = 20 * 0.5 = 10 validation\n",
        "\n",
        "Set Image size to 100*100 * 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0caQZAxaBmW7",
        "outputId": "bee65284-eb85-4f55-c281-4b7bb9d6ef52"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (100, 100)\n",
        "COLOR_MODE = 'rgb' #\"grayscale\"  #  \"rgb\"\n",
        "Color_Num = 3\n",
        "Valid_ratio = 0.5\n",
        "\n",
        "train_dataset_CNN = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = \"training\",\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )\n",
        "Validation_dataset_CNN = image_dataset_from_directory(PATH,\n",
        "                                             seed = 1234,                                           \n",
        "                                             color_mode = COLOR_MODE, \n",
        "                                             validation_split = Valid_ratio,\n",
        "                                             subset = 'validation',\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             label_mode ='categorical',\n",
        "                                             )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for training.\n",
            "Found 80 files belonging to 4 classes.\n",
            "Using 40 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfvMagBrBmW-"
      },
      "source": [
        "Normalize data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RThOkmcXBmW_"
      },
      "source": [
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#reshape_layer = tf.keras.layers.Reshape(3, 4)\n",
        "train_normalized_ds_CNN = train_dataset_CNN.map(lambda x, y: (normalization_layer(x), y))\n",
        "validation_normalized_ds_CNN = Validation_dataset_CNN.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDqfMm1BBmW_"
      },
      "source": [
        "Convert normalized dataset to images and labels in nparray "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPgS-EYwBmXA"
      },
      "source": [
        "train_images_CNN,train_labels_CNN  = list(train_normalized_ds_CNN.as_numpy_iterator())[0]\n",
        "validate_images_CNN,validate_labels_CNN  = list(validation_normalized_ds_CNN.as_numpy_iterator())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "q4hi0DhCx1-9",
        "outputId": "8b3676fd-b07d-4843-bf48-50c6ffb73921"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l1,l2\n",
        "CNN_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=((IMG_SIZE[0],IMG_SIZE[1],Color_Num))),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), strides=2, padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(4, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "CNN_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7eddc9f1f16b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m CNN_model = keras.Sequential(\n\u001b[1;32m      6\u001b[0m     [\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mColor_Num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'IMG_SIZE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX1iUIpLztDl"
      },
      "source": [
        "Compile and Fit CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpKbqFu-zrk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d203b1ef-8cf4-4298-9715-c93b38ddb94c"
      },
      "source": [
        "CNN_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "CNN_model.fit(x= train_images_CNN, y= train_labels_CNN,\n",
        "                epochs=50,\n",
        "                batch_size=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(validate_images_CNN,validate_labels_CNN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 2s 116ms/step - loss: 1.3961 - accuracy: 0.2167 - val_loss: 1.3920 - val_accuracy: 0.1750\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 1.3493 - accuracy: 0.3600 - val_loss: 1.3916 - val_accuracy: 0.1750\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.2739 - accuracy: 0.3100 - val_loss: 1.3755 - val_accuracy: 0.3000\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.1659 - accuracy: 0.4433 - val_loss: 1.2845 - val_accuracy: 0.3750\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.0173 - accuracy: 0.6567 - val_loss: 1.1280 - val_accuracy: 0.4000\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.8305 - accuracy: 0.7833 - val_loss: 0.9519 - val_accuracy: 0.5000\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.7726 - accuracy: 0.6967 - val_loss: 0.8517 - val_accuracy: 0.6000\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.6597 - accuracy: 0.7633 - val_loss: 0.8775 - val_accuracy: 0.5250\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.4292 - accuracy: 0.8333 - val_loss: 0.7022 - val_accuracy: 0.6500\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4460 - accuracy: 0.8367 - val_loss: 0.8202 - val_accuracy: 0.6000\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4437 - accuracy: 0.8467 - val_loss: 0.8957 - val_accuracy: 0.5500\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3182 - accuracy: 0.8900 - val_loss: 0.7244 - val_accuracy: 0.7250\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4384 - accuracy: 0.8633 - val_loss: 0.6824 - val_accuracy: 0.7000\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3619 - accuracy: 0.8700 - val_loss: 1.2008 - val_accuracy: 0.5750\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3286 - accuracy: 0.8733 - val_loss: 0.5881 - val_accuracy: 0.7500\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3386 - accuracy: 0.8433 - val_loss: 1.0031 - val_accuracy: 0.5500\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2790 - accuracy: 0.8933 - val_loss: 0.6851 - val_accuracy: 0.6750\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2136 - accuracy: 0.9333 - val_loss: 0.7394 - val_accuracy: 0.6750\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2286 - accuracy: 0.9667 - val_loss: 0.7415 - val_accuracy: 0.6750\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1571 - accuracy: 0.9400 - val_loss: 0.7098 - val_accuracy: 0.6750\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2501 - accuracy: 0.9200 - val_loss: 0.6946 - val_accuracy: 0.7000\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1792 - accuracy: 0.9267 - val_loss: 0.7094 - val_accuracy: 0.7250\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1032 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.6500\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1190 - accuracy: 0.9733 - val_loss: 0.6546 - val_accuracy: 0.7500\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0965 - accuracy: 1.0000 - val_loss: 0.6297 - val_accuracy: 0.7750\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 1.0046 - val_accuracy: 0.6750\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1013 - accuracy: 0.9800 - val_loss: 1.0095 - val_accuracy: 0.7000\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1387 - accuracy: 0.9733 - val_loss: 0.7757 - val_accuracy: 0.7000\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0840 - accuracy: 0.9800 - val_loss: 1.0722 - val_accuracy: 0.7000\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 0.8000\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1264 - accuracy: 0.9533 - val_loss: 0.7771 - val_accuracy: 0.6750\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 1.5004 - val_accuracy: 0.6500\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2174 - accuracy: 0.9000 - val_loss: 0.7279 - val_accuracy: 0.7750\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1387 - accuracy: 0.9133 - val_loss: 0.9496 - val_accuracy: 0.7250\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 0.9900 - val_loss: 1.7903 - val_accuracy: 0.5250\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.8026 - val_accuracy: 0.7000\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0461 - accuracy: 0.9833 - val_loss: 0.7969 - val_accuracy: 0.7250\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.8604 - val_accuracy: 0.7000\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 1.1928 - val_accuracy: 0.7000\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 1.1378 - val_accuracy: 0.6750\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0298 - accuracy: 0.9733 - val_loss: 0.9017 - val_accuracy: 0.7500\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.8209 - val_accuracy: 0.7000\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0346 - accuracy: 0.9833 - val_loss: 1.0238 - val_accuracy: 0.7000\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.3680 - val_accuracy: 0.6250\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 1.0845 - val_accuracy: 0.6500\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.8038 - val_accuracy: 0.7250\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.8541 - val_accuracy: 0.7500\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0789 - accuracy: 0.9733 - val_loss: 1.4433 - val_accuracy: 0.6750\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1166 - accuracy: 0.9567 - val_loss: 1.2152 - val_accuracy: 0.7000\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1732 - accuracy: 0.8967 - val_loss: 1.1452 - val_accuracy: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2bc97d4d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVmq3jaqJ_z5",
        "outputId": "6c11df3e-eb81-4661-ee4c-c8bd4ade3365"
      },
      "source": [
        "scores = CNN_model.evaluate(validate_images_CNN,validate_labels_CNN, verbose=1) \n",
        "print(\"CNN Accuracy: \", scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 1.1452 - accuracy: 0.7000\n",
            "CNN Accuracy:  0.699999988079071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Np8gn7gf3_"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "#Accuracy:\n",
        "\n",
        "Pretrained VGG16 give accuracy of 0.825 on validation images.\n",
        "\n",
        "Pretrained GoogleNet (InceptionV3) give accuracy of 0.975 on validation images\n",
        "\n",
        "Use VGG16 as Transfer model give accuracy of 0.9 on validation data after fine tuning.\n",
        "\n",
        "DCT2 traditional with KNN give accuracy of  0.55 on validation data, which mean we need to choose other tradional algorithm to extract feature.\n",
        "\n",
        "CNN Model give accuracy of 0.7 on validation images\n",
        "\n",
        "#Timing:\n",
        "\n",
        "DCT2,  Pretrained models (VGG16 and GoogleNet) have about no time to extract features and KNN is fast algorithm. \n",
        "\n",
        "Transfer Modeling is fast when freeze the weights, but fine tuning take some time.\n",
        "\n",
        "CNN Model is the consume a little bit time.\n",
        "\n",
        "# Final\n",
        "Using transfer modeling with fine tuning enhanced VGG16 but GoogleNet (InceptionV3) give better accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "329Kwy_Z4PD0"
      },
      "source": [
        "#featuresArr = []\n",
        "#for x in train_images:\n",
        "#  features = model.predict(x)\n",
        "#  features_reduce =  features.squeeze()\n",
        "#  featuresArr.append(features_reduce)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onmn4Iosint_"
      },
      "source": [
        "#from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "#urllib.request.urlretrieve(\"https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg\", \"sample.jpg\")\n",
        "#img_path = 'sample.jpg'\n",
        "#img = image.load_img(img_path, target_size=(224, 224))\n",
        "#x = image.img_to_array(img)\n",
        "#x = np.expand_dims(x, axis=0)\n",
        "#x = preprocess_input(x)\n",
        "\n",
        "#features = model.predict(x)\n",
        "#print('Predicted:', decode_predictions(features, top=3)[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}